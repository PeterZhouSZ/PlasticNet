{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Baby PlasticNet.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sreyafrancis/PlasticNet/blob/master/Baby_PlasticNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JyeWGsl6vjJ8",
        "colab_type": "text"
      },
      "source": [
        "# Baby PlasticNet ... the first steps\n",
        "### Point cloud shape representation\n",
        "\n",
        "This network is an adaptation of PointNet into a regression encoder.  It's objective is to take in a point cloud of model and find the paramaters to re-create that model.  The main objective is to transition from an object's discrete denotation to it's mathematical representation.  This atempt is done via unsupervised learning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0CipC2TBwq8I",
        "colab_type": "text"
      },
      "source": [
        "First step is to tackle a simple ellipsoid and find it's parameters.  The encoder will output the parameters of a triaxial ellipsoid with least square from the original parameters as the error function.\n",
        "In order to consider all equivalent parametrisations, the paramaters to find are 3 sets of paramters of tranfomation matrices : \\\n",
        "*   Affine (shearing and squeezing)\n",
        "*   Projection\n",
        "*   Rotation\n",
        "\n",
        "This can only be used for training the encoder however, as in a test set or later when the encoder will be used in higher context, those parameters will be unknown.  At that time, the error will defined by the Hausdorff between source mesh and output and represents the exact reconstruction error.  This means the gradient is computed manually (very slow)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h7aWw8KSeouK",
        "colab_type": "text"
      },
      "source": [
        "####Unsolved issues\n",
        "\t- No Primitive labeling for ShapeNet items\n",
        "\t\t- Can label belonging to the same item in a scene\n",
        "\t- Variable point bag sizes\n",
        "\t- Surrogate manual gradient is very slow\n",
        "\t\n",
        "###### Some links related to labeling\n",
        "\n",
        "- http://shapenet.cs.stanford.edu/shapenet/obj-zip/ShapeNetCore.v2-old/shapenet/tex/2016Proposal/2016shapenet_main.pdf\n",
        "- https://www.groundai.com/project/physical-primitive-decomposition/1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w7aNJ9EBVG1W",
        "colab_type": "text"
      },
      "source": [
        "###Libraries :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gy3MoePDCx92",
        "colab_type": "code",
        "outputId": "3449c396-6570-47a1-96c8-2ab546a68e6b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        }
      },
      "source": [
        "pip install git+git://github.com/fwilliams/point-cloud-utils\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+git://github.com/fwilliams/point-cloud-utils\n",
            "  Cloning git://github.com/fwilliams/point-cloud-utils to /tmp/pip-req-build-sh0e3504\n",
            "  Running command git clone -q git://github.com/fwilliams/point-cloud-utils /tmp/pip-req-build-sh0e3504\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from point-cloud-utils==0.8.0) (1.18.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from point-cloud-utils==0.8.0) (1.4.1)\n",
            "Building wheels for collected packages: point-cloud-utils\n",
            "  Building wheel for point-cloud-utils (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for point-cloud-utils: filename=point_cloud_utils-0.8.0-cp36-cp36m-linux_x86_64.whl size=2093233 sha256=9554427a680d3640570aafbb65358d0cc69491c93268232cc35ed674d3f633b8\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-zkiighf1/wheels/1b/49/41/aa9b652a2ae796bec5f5e7a412819a7a6911f100f9bf211f32\n",
            "Successfully built point-cloud-utils\n",
            "Installing collected packages: point-cloud-utils\n",
            "Successfully installed point-cloud-utils-0.8.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PEK38iyq5GUS",
        "colab_type": "code",
        "outputId": "80312ee1-48c3-4e31-9804-6d203e324079",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        }
      },
      "source": [
        "pip install mathutils"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting mathutils\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fb/00/774d1f29b7fc0eff0198668a33ecda8ee66a71e9f3ea5d34fe6238e01538/mathutils-2.81.2.tar.gz (226kB)\n",
            "\r\u001b[K     |█▌                              | 10kB 19.7MB/s eta 0:00:01\r\u001b[K     |███                             | 20kB 1.8MB/s eta 0:00:01\r\u001b[K     |████▍                           | 30kB 2.1MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 40kB 1.6MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 51kB 1.9MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 61kB 2.2MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 71kB 2.4MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 81kB 2.6MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 92kB 2.9MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 102kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████                | 112kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 122kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 133kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 143kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 153kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 163kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 174kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 184kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 194kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 204kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 215kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 225kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 235kB 2.8MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: mathutils\n",
            "  Building wheel for mathutils (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mathutils: filename=mathutils-2.81.2-cp36-cp36m-linux_x86_64.whl size=534033 sha256=2c85b49827b3397f4e6f85ba2ba0be8bfa80601f661d86b081d1216642c2e9d3\n",
            "  Stored in directory: /root/.cache/pip/wheels/74/34/b8/80048950a103837b3af2cc9d2833bdf75361855022de3a0299\n",
            "Successfully built mathutils\n",
            "Installing collected packages: mathutils\n",
            "Successfully installed mathutils-2.81.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jv2j8cgIKIYB",
        "colab_type": "code",
        "outputId": "5d07f1d8-0974-44d0-b3b5-25f4d3d8cad6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import numpy as np\n",
        "from math import radians\n",
        "import random\n",
        "import time\n",
        "import os\n",
        "import copy\n",
        "\n",
        "import point_cloud_utils as pcu\n",
        "from scipy.spatial import KDTree\n",
        "\n",
        "from itertools import product\n",
        "from mathutils import Vector\n",
        "from mathutils import Matrix\n",
        "from mathutils import geometry\n",
        "# from collections import Counter\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "print(torch.__version__)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.4.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pK_21irtVLqD",
        "colab_type": "text"
      },
      "source": [
        "## Plastic Block class :\n",
        "\n",
        "(First : this data structure uses arrays to keep vertices' id\n",
        "        What I thought was a better idea than a linked list turns out to be a           terrible idea\n",
        "        It should be redone that way.... )"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "neZpVcs6ltIj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class PlasticBlock:\n",
        "\n",
        "    class Point:\n",
        "        def __init__(self, p_id, x, y, z):\n",
        "            self.p_id = p_id\n",
        "            self.coord = [x, y, z]\n",
        "\n",
        "    def add_vertex(self, x, y, z):\n",
        "        p_id = self.vertices.__len__()\n",
        "        self.vertices.append(Vector([x, y, z]))\n",
        "        p = PlasticBlock.Point(p_id, x, y, z)\n",
        "\n",
        "        return p\n",
        "\n",
        "    def __init__(self, parameters=None,\n",
        "                 eccentricity: int = random.randint(0, 4),\n",
        "                 rot_ecc: int = random.randint(0, 3),\n",
        "                 rot_mag: float = np.random.beta(2, 5),\n",
        "                 shear_ecc: int = random.randint(0, 4),\n",
        "                 shear_mag = random.randint(0, 3),\n",
        "                 smoothness=6):\n",
        "\n",
        "        if parameters is None :\n",
        "            parameters = self.random_sample(eccentricity, rot_ecc, rot_mag, shear_ecc, shear_mag)\n",
        "\n",
        "        # list tru all radius of curvature parameters and apply cap\n",
        "        for i in range(0, len(parameters), 2):\n",
        "            parameters[i] = min(parameters[i], 1)\n",
        "            if parameters[i] > .95:\n",
        "                parameters[i] = 1\n",
        "            if parameters[i] < .05:\n",
        "                parameters[i] = .05\n",
        "\n",
        "        self.source = parameters\n",
        "        self.params = parameters[:40].reshape((-1, 2))\n",
        "        self.affine = parameters[40:]\n",
        "        self.sub = max(int(smoothness), 2)\n",
        "        self.vertices = []\n",
        "        self.edge_list = []\n",
        "        self.face_list = []\n",
        "        # control points\n",
        "        #   array of 8 control points\n",
        "        self.control_points = []\n",
        "        for p_id in range(8):\n",
        "            p = self.add_vertex((((p_id >> 2) & 1) * -2) + 1, ((((p_id & -6) >> 1) & 1) * -2) + 1, (((p_id & -7) & 1) * -2) + 1)\n",
        "            self.control_points.append(p)\n",
        "\n",
        "        # array of 12 meta edges\n",
        "        self.m_edges = np.empty((3, 4), dtype=object)\n",
        "        # again edges' id are one-hot encoded related to control point's code\n",
        "        # it's simpler to list them manually, but a bitwise function can do it\n",
        "        # i is the bit position to be removed, index is read from the 2 remaining bits\n",
        "        idx = [[4, 0], [5, 1], [6, 2], [7, 3]]\n",
        "        idy = [[2, 0], [3, 1], [6, 4], [7, 5]]\n",
        "        idz = [[1, 0], [3, 2], [5, 4], [7, 6]]\n",
        "        for i, listi in enumerate([idx, idy, idz]):\n",
        "            for j, index in enumerate(listi):\n",
        "                self.m_edges[i][j] = PlasticBlock.Edge(self.control_points[index[0]], self.control_points[index[1]])\n",
        "                p = self.add_vertex(self.m_edges[i][j].coord[0], self.m_edges[i][j].coord[1], self.m_edges[i][j].coord[2])\n",
        "                self.m_edges[i][j].point = p\n",
        "                self.m_edges[i][j].p_id = p.p_id\n",
        "                self.m_edges[i][j].curve = self.params[p.p_id]\n",
        "\n",
        "        # complete dual graph with faces\n",
        "        self.faces = []\n",
        "        # x edges :\n",
        "        self.add_face(self.m_edges[1][0], self.m_edges[1][1], self.m_edges[2][0], self.m_edges[2][1])\n",
        "        self.add_face(self.m_edges[1][2], self.m_edges[1][3], self.m_edges[2][2], self.m_edges[2][3])\n",
        "        # y edges:\n",
        "        self.add_face(self.m_edges[0][0], self.m_edges[0][1], self.m_edges[2][0], self.m_edges[2][2])\n",
        "        self.add_face(self.m_edges[0][2], self.m_edges[0][3], self.m_edges[2][1], self.m_edges[2][3])\n",
        "        # z edges:\n",
        "        self.add_face(self.m_edges[0][0], self.m_edges[0][2], self.m_edges[1][0], self.m_edges[1][2])\n",
        "        self.add_face(self.m_edges[0][1], self.m_edges[0][3], self.m_edges[1][1], self.m_edges[1][3])\n",
        "\n",
        "        for c, point in enumerate(self.control_points):\n",
        "            ex = self.m_edges[0][idx.index([x for x in idx if c in x][0])]\n",
        "            ey = self.m_edges[1][idy.index([x for x in idy if c in x][0])]\n",
        "            ez = self.m_edges[2][idz.index([x for x in idz if c in x][0])]\n",
        "            self.control_points[c] = PlasticBlock.CtrPoint(self, c, point, ex, ey, ez, self.params[point.p_id], self.sub)\n",
        "\n",
        "        self.get_edges()\n",
        "        self.get_faces()\n",
        "\n",
        "        for c in self.control_points:\n",
        "            self.deform_sphere(c)\n",
        "\n",
        "        for e in self.m_edges.reshape(12):\n",
        "            self.deform_cylinder(e)\n",
        "\n",
        "        self.apply_affine(self.affine)\n",
        "        # Counter can check for duplicates\n",
        "        # print(Counter(tuple(item) for item in self.edge_list))\n",
        "        a = 1\n",
        "\n",
        "    class FacePoint:\n",
        "        # Create a face with 4 co-planar edges\n",
        "        def __init__(self, e1, e2, e3, e4):\n",
        "            self.coord = (np.equal(e1.coord, e2.coord) & np.equal(e3.coord, e4.coord)).astype(float) * e1.coord\n",
        "            self.point = None\n",
        "            self.p_id = -1\n",
        "\n",
        "    def add_face(self, e1, e2, e3, e4):\n",
        "        face = PlasticBlock.FacePoint(e1, e2, e3, e4)\n",
        "        face.point = self.add_vertex(face.coord[0], face.coord[1], face.coord[2])\n",
        "        face.p_id = face.point.p_id\n",
        "        self.faces.append(face)\n",
        "\n",
        "        for e in [e1, e2, e3, e4]:\n",
        "            e.add_face_e(face)\n",
        "\n",
        "        return face\n",
        "\n",
        "    class CtrPoint:\n",
        "        # Control point id is defined by one-hot vector\n",
        "        # 0 = + , 1 = - for x,y,z\n",
        "        # ie: ctr_point 3 is 011 :  [1,-1,-1]\n",
        "        def __init__(self, block, p_id, point, ex, ey, ez, curve, sub=2):\n",
        "            self.p_id = p_id\n",
        "            self.ex = ex\n",
        "            self.ey = ey\n",
        "            self.ez = ez\n",
        "            self.coord = point.coord\n",
        "            self.curve = curve\n",
        "            # 3 free points array between control point and edge point\n",
        "            self.x_list = []\n",
        "            self.y_list = []\n",
        "            self.z_list = []\n",
        "\n",
        "            ## Todo  solve mixing of cylinder radius and sphere radius corners...\n",
        "\n",
        "            # Radius consensus among 3 possibilities. (maximum influence is chosen, but could also be averaged)\n",
        "\n",
        "            a_max = np.argmax([curve[0]*curve[1], ex.curve[0]*ex.curve[1], ey.curve[0]*ey.curve[1], ez.curve[0]*ez.curve[1]])\n",
        "            self.r = [curve[0], ex.curve[0], ey.curve[0], ez.curve[0]][a_max]\n",
        "            for i, r in enumerate([curve, ex.curve, ey.curve, ez.curve]):\n",
        "                if i == a_max:\n",
        "                    continue\n",
        "                r[0] = 1\n",
        "                r[1] = 0\n",
        "\n",
        "            self.step_x = (1 / (sub - (1 if (self.r == 1) else 0))) * self.r\n",
        "            self.step_y = (1 / (sub - (1 if (self.r == 1) else 0))) * self.r\n",
        "            self.step_z = (1 / (sub - (1 if (self.r == 1) else 0))) * self.r\n",
        "            if sub > 2:\n",
        "                for i in range(1, sub-1):\n",
        "                    self.x_list.append(block.add_vertex(self.coord[0]*(1 - self.step_x*i), self.coord[1], self.coord[2]).p_id)\n",
        "                    self.y_list.append(block.add_vertex(self.coord[0], self.coord[1]*(1 - self.step_y*i), self.coord[2]).p_id)\n",
        "                    self.z_list.append(block.add_vertex(self.coord[0], self.coord[1], self.coord[2]*(1 - self.step_z*i)).p_id)\n",
        "            self.a_xy = self._build_quadrant(block, self.x_list, self.y_list, ex, ey, self.step_x, sub)\n",
        "            self.a_xz = self._build_quadrant(block, self.x_list, self.z_list, ex, ez, self.step_y, sub)\n",
        "            self.a_yz = self._build_quadrant(block, self.y_list, self.z_list, ey, ez, self.step_z, sub)\n",
        "\n",
        "            for e in [ex, ey, ez]:\n",
        "                # find value in control point coord at m_edge's 0 coord\n",
        "                # (all m_edge have one 0 coord : ie [-1 0 1] is an m_edge\n",
        "                if self.coord[np.where([e.coord == 0][0])[0][0]] < 0:\n",
        "                    e.ctp_n = self\n",
        "                else:\n",
        "                    e.ctp_p = self\n",
        "\n",
        "        # not to forget : each border of a quadrant array is overlapping the border of another quadrant array\n",
        "        def _build_quadrant(self, block, e1_list, e2_list, e1, e2, step, sub=2):\n",
        "            a = np.empty((sub, sub), dtype=int)\n",
        "            if sub > 2:\n",
        "                for i in range(1, sub-1):\n",
        "                    a[0, i] = e2_list[i-1]\n",
        "                    a[i, 0] = e1_list[i-1]\n",
        "            facepoint = None\n",
        "            for f1, f2 in product(e1.faces, e2.faces):\n",
        "                if f1 == f2:\n",
        "                    facepoint = f1\n",
        "                    break\n",
        "            a[ 0, 0] = self.p_id\n",
        "            a[ 0,-1] = e2.p_id\n",
        "            a[-1, 0] = e1.p_id\n",
        "            a[-1,-1] = facepoint.p_id\n",
        "\n",
        "            if sub > 2:\n",
        "                for i in range(1, sub-1):\n",
        "                    for j in range(1, sub - 1):\n",
        "                        coord = self.coord - ((self.coord - e1.coord)*i*step + (self.coord - e2.coord)*j*step)\n",
        "                        a[i, j] = block.add_vertex(coord[0], coord[1], coord[2]).p_id\n",
        "\n",
        "                # get the inner edge (edge's mid point to face center)\n",
        "                (a[-1, :])[1:-1] = e1.get_dual_edge(block, facepoint, step, sub)\n",
        "                (a[:, -1])[1:-1] = e2.get_dual_edge(block, facepoint, step, sub)\n",
        "\n",
        "            return a\n",
        "\n",
        "    class Edge:\n",
        "        def __init__(self, ctp_n, ctp_p):\n",
        "            self.ctp_n = ctp_n\n",
        "            self.ctp_p = ctp_p\n",
        "            self.coord = (np.equal(ctp_n.coord, ctp_p.coord).astype(int) * ctp_p.coord).astype(float)\n",
        "            self.p_id = -1\n",
        "            self.point = None\n",
        "            self.curve = []\n",
        "            # self faces are the 2 that both points have in common\n",
        "            self.faces = []\n",
        "            self.dual_0 = []\n",
        "            self.dual_1 = []\n",
        "\n",
        "        def get_dual_edge(self, block, face, step, sub=2):\n",
        "            if sub == 2:\n",
        "                return []\n",
        "            if face == self.faces[0]:\n",
        "                sec = self.dual_0\n",
        "            else:\n",
        "                sec = self.dual_1\n",
        "            coord = np.copy(self.coord)\n",
        "            axis = np.where(self.coord - face.coord)[0][0]\n",
        "            if len(sec) == 0:\n",
        "                for i in range(sub - 2):\n",
        "                    coord[axis] = self.coord[axis] * (1 - step*(i+1))\n",
        "                    sec.append(block.add_vertex(coord[0], coord[1], coord[2]).p_id)\n",
        "                if face == self.faces[0]:\n",
        "                    self.dual_0 = sec\n",
        "                else:\n",
        "                    self.dual_1 = sec\n",
        "            else:\n",
        "                for i in range(sub - 2):\n",
        "                    coord[axis] = self.coord[axis] * (1 - step*(i+1))\n",
        "                    block.vertices[sec[i]] = ((block.vertices[sec[i]] + Vector(coord))/2)\n",
        "            return sec\n",
        "\n",
        "        def add_face_e(self, face):\n",
        "            if face not in self.faces:\n",
        "                self.faces.append(face)\n",
        "\n",
        "    def get_edges(self):\n",
        "        # use Quand-Edge structure to loop tru all unique edges and stitch them\n",
        "\n",
        "        for e in self.m_edges.reshape(12):\n",
        "            # consider only positive direction of m_edge\n",
        "            ctp_n = e.ctp_n\n",
        "            ctp_p = e.ctp_p\n",
        "            for quadrant in [ctp_n.a_xy, ctp_n.a_xz, ctp_n.a_yz]:\n",
        "                if quadrant[0, -1] == e.p_id:\n",
        "                    edges_a = quadrant[:, -1][:]\n",
        "                    self.edge_list.extend([[x, y] for x, y in zip(edges_a, edges_a[1:])])\n",
        "                if quadrant[-1, 0] == e.p_id:\n",
        "                    edges_a = quadrant[-1, :].T[:]\n",
        "                    self.edge_list.extend([[x, y] for x, y in zip(edges_a, edges_a[1:])])\n",
        "\n",
        "            for c in [ctp_n, ctp_p]:\n",
        "                for quadrant in [c.a_xy, c.a_xz, c.a_yz]:\n",
        "                    if quadrant[0, -1] == e.p_id:\n",
        "                        edges_a = quadrant[0, :][:]\n",
        "                        self.edge_list.extend([[x, y] for x, y in zip(edges_a, edges_a[1:])])\n",
        "                        break\n",
        "                    if quadrant[-1, 0] == e.p_id:\n",
        "                        edges_a = quadrant[:, 0].T[:]\n",
        "                        self.edge_list.extend([[x, y] for x, y in zip(edges_a, edges_a[1:])])\n",
        "                        break\n",
        "\n",
        "        # Convolution for each quadrant to list free edges\n",
        "        if self.sub > 2:\n",
        "            for c in self.control_points:\n",
        "                for quadrant in [c.a_xy, c.a_xz, c.a_yz]:\n",
        "                    self.quadrant_conv(quadrant)\n",
        "\n",
        "    def quadrant_conv(self, quadrant):\n",
        "        for i in range(1, self.sub - 1):\n",
        "            for j in range(1, self.sub - 1):\n",
        "                self.edge_list.append([quadrant[i][j - 1], quadrant[i][j]])\n",
        "                self.edge_list.append([quadrant[i][j], quadrant[i - 1][j]])\n",
        "                if j == self.sub - 2:\n",
        "                    self.edge_list.append([quadrant[i][j], quadrant[i][j + 1]])\n",
        "                if i == self.sub - 2:\n",
        "                    self.edge_list.append([quadrant[i][j], quadrant[i + 1][j]])\n",
        "\n",
        "    def get_faces(self):\n",
        "        # Convolution to list all faces of each quadrant of each control point\n",
        "        for c in self.control_points:\n",
        "            for quadrant in [c.a_xy, c.a_xz, c.a_yz]:\n",
        "                for i in range(1, self.sub):\n",
        "                    for j in range(1, self.sub):\n",
        "                        self.face_list.append([quadrant[i - 1][j - 1], quadrant[i - 1][j],\n",
        "                                                quadrant[i][j], quadrant[i][j-1]])\n",
        "\n",
        "    def deform_sphere(self, c: CtrPoint):\n",
        "        c_id = c.p_id\n",
        "        r = self.params[c_id, 0]\n",
        "        mag = self.params[c_id, 1]\n",
        "        if r == 0 or mag == 0:\n",
        "            return\n",
        "        control = self.vertices[c_id].copy()\n",
        "\n",
        "        def _inrange(vertex, control):\n",
        "            vc = self.vertices[vertex] - control\n",
        "            if abs(vc[0]) <= r and abs(vc[1]) <= r and abs(vc[2]) <= r:\n",
        "                return True\n",
        "            return False\n",
        "        # parameters should be a vector of length 40 reshaped into :\n",
        "        # list of corner spherisations [r, magnitude]\n",
        "        # for each corner, apply it's spherical transform\n",
        "        vertices = []\n",
        "        for quadrant in [c.a_xy, c.a_xz, c.a_yz]:\n",
        "            vertices.extend(quadrant.reshape((1, -1)))\n",
        "\n",
        "        center = self.vertices[c_id]*(1 - r)\n",
        "        vertices = np.unique(vertices)\n",
        "        for v in vertices:\n",
        "            if _inrange(v, control):\n",
        "                self.vertices[v] = self.round_corner(center, self.vertices[v], r, mag)\n",
        "\n",
        "        for e in self.m_edges.reshape((1, -1))[0]:\n",
        "            self.clean_dual_edge(e)\n",
        "\n",
        "    def deform_cylinder(self, e: Edge):\n",
        "        # Cylindracisations\n",
        "        # for each edge, apply it's cylindrical transform [r, magnitude]\n",
        "        e_id = e.p_id\n",
        "        r = self.params[e_id, 0]\n",
        "        mag = self.params[e_id, 1]\n",
        "        if r == 0 or mag == 0:\n",
        "            return\n",
        "        mid = Vector(e.coord)\n",
        "        mask = np.array(mid, dtype=bool)\n",
        "        normal = Vector(np.invert(mask).astype(dtype=float))\n",
        "        plane = np.where(mask)[0]\n",
        "\n",
        "        def _inrange(vertex, ctrl):\n",
        "            vc = self.vertices[vertex] - ctrl\n",
        "            if abs(vc[0]) <= r and abs(vc[1]) <= r and abs(vc[2]) <= r:\n",
        "                return True\n",
        "            return False\n",
        "\n",
        "        vertices = []\n",
        "        for c in [e.ctp_n, e.ctp_p]:\n",
        "            # filter quadrant normal to edge\n",
        "            for quadrant in [[c.a_yz, c.a_xz, c.a_xy][i] for i in plane]:\n",
        "                vertices.extend(quadrant.reshape((1, -1)))\n",
        "        vertices = np.unique(vertices)\n",
        "\n",
        "        c_p = (mid + normal)\n",
        "        c_n = (mid - normal)\n",
        "        for v in vertices:\n",
        "            control = geometry.intersect_line_plane(c_p, c_n, self.vertices[v], normal)\n",
        "            center = geometry.intersect_line_plane(normal, normal*-2, self.vertices[v], normal)\n",
        "            center = center + (control - center)*(1 - r)\n",
        "            if _inrange(v, control):\n",
        "                self.vertices[v] = self.round_corner(center, self.vertices[v], r, mag)\n",
        "\n",
        "        vertices = []\n",
        "        for c in [e.ctp_n, e.ctp_p]:\n",
        "            # filter quadrant of planar curve\n",
        "            for quadrant in [[c.a_yz, c.a_xz, c.a_xy][i] for i in np.where(normal)[0]]:\n",
        "                vertices.extend(quadrant[1:, 1:].reshape((1, -1)))\n",
        "        vertices = np.unique(vertices)\n",
        "\n",
        "        for v in vertices:\n",
        "            control = geometry.intersect_line_plane(c_p, c_n, self.vertices[v], normal)\n",
        "            center = geometry.intersect_line_plane(normal, normal*-2, self.vertices[v], normal)\n",
        "            center = center + (control - center)*(1 - r)\n",
        "            if (control - self.vertices[v]).length < (control - center).length:\n",
        "                #continue\n",
        "                self.vertices[v] = self.vertices[v] + (center - self.vertices[v])*.3*(center - self.vertices[v]).length\n",
        "\n",
        "        for e in self.m_edges.reshape((1, -1))[0]:\n",
        "            self.clean_dual_edge(e)\n",
        "\n",
        "    def round_corner(self, origin: Vector, vector: Vector, r: float, magnitude: float):\n",
        "        # project unto sphere of radius r at x0,y0,z0\n",
        "        # mask ensures points on axis planes remain in that plane (numerical accuracy issue)\n",
        "        mask = np.array(vector, dtype=bool)\n",
        "        q = vector - origin\n",
        "        if q.length == 0:\n",
        "            return\n",
        "        p = origin + q*(r/q.length)\n",
        "        p = (p - vector)*magnitude + vector\n",
        "\n",
        "        return Vector((p[0]*mask[0], p[1]*mask[1], p[2]*mask[2]))\n",
        "\n",
        "    def clean_dual_edge(self, e):\n",
        "        \"\"\"\n",
        "            Get a dual list from an edge\n",
        "            find the 2 quadrants sharing it\n",
        "        \"\"\"\n",
        "        if self.sub > 2:\n",
        "            for dual in [e.dual_0, e.dual_1]:\n",
        "                quads = []\n",
        "                for c in [e.ctp_n, e.ctp_p]:\n",
        "                    for quadrant in [c.a_xy, c.a_xz, c.a_yz]:\n",
        "                        if dual[0] == quadrant[-1,1]:\n",
        "                            quads.append(quadrant)\n",
        "                            break\n",
        "                        if dual[0] == quadrant[1,-1]:\n",
        "                            quads.append(quadrant.T)\n",
        "                            break\n",
        "                for i in range(self.sub - 1):\n",
        "                    a = Vector(e.coord)#self.vertices[quads[0][-1][i]]\n",
        "                    b = self.vertices[quads[0][-2][i]]\n",
        "                    c = self.vertices[quads[1][-2][i]]\n",
        "                    mask = np.array(a, dtype=bool)\n",
        "                    n = np.invert(mask).astype(dtype=float)\n",
        "                    p = geometry.intersect_line_plane(c, b, a, Vector((n[0], n[1], n[2])))\n",
        "                    self.vertices[quads[0][-1][i]] = Vector((p[0]*mask[0], p[1]*mask[1], p[2]*mask[2]))\n",
        "\n",
        "    def apply_affine(self, affine_params):\n",
        "        # get 12 parameters of an affine transformation matrix\n",
        "        # [scale x, y, z, shear xy, xz, yx, yz, zx, zy, rotation x, y, z]\n",
        "        S = Matrix.Identity(3)\n",
        "\n",
        "        # scale x, y, z\n",
        "        S[0][0] = max(0.05, affine_params[0])\n",
        "        S[1][1] = max(0.05, affine_params[1])\n",
        "        S[2][2] = max(0.05, affine_params[2])\n",
        "\n",
        "        # shear\n",
        "        S[1][0] = affine_params[3]\n",
        "        S[2][0] = affine_params[4]\n",
        "        S[0][1] = affine_params[5]\n",
        "        S[1][2] = affine_params[6]\n",
        "        S[0][2] = affine_params[7]\n",
        "        S[1][2] = affine_params[8]\n",
        "\n",
        "        Rx = Matrix.Rotation(radians(affine_params[9]), 3, 'X')\n",
        "        Ry = Matrix.Rotation(radians(affine_params[10]), 3, 'Y')\n",
        "        Rz = Matrix.Rotation(radians(affine_params[11]), 3, 'Z')\n",
        "        R = Rx @ Ry @ Rz\n",
        "\n",
        "        T = R @ S\n",
        "        for i in range(len(self.vertices)):\n",
        "            self.vertices[i] = T @ self.vertices[i]\n",
        "\n",
        "    # Returns a triangulated obj array format\n",
        "    def get_obj(self):\n",
        "        # get edge list\n",
        "        # get face list\n",
        "        # get vertex list\n",
        "        vertices = np.zeros((len(self.vertices), 3))\n",
        "        for i in range(len(self.vertices)):\n",
        "            vertices[i, :] = [self.vertices[i].x, self.vertices[i].y, self.vertices[i].z]\n",
        "\n",
        "        edges = self.edge_list.copy()\n",
        "        faces = []\n",
        "        # triangulate the quadmesh\n",
        "        for f in range(len(self.face_list)):\n",
        "            faces.append([self.face_list[f][0], self.face_list[f][1], self.face_list[f][2]])\n",
        "            faces.append([self.face_list[f][2], self.face_list[f][3], self.face_list[f][0]])\n",
        "            edges.append([self.face_list[f][0], self.face_list[f][2]])\n",
        "\n",
        "        return vertices, np.asarray(edges), np.asarray(faces)\n",
        "\n",
        "    def random_sample(self, eccentricity: int, rot_ecc: int, rot_mag: float, shear_ecc: int, shear_mag):\n",
        "        # Eccentricity is parameter of block's variance away from a primitive\n",
        "        # defined as eccentricity : #of defining components\n",
        "        # return the parameter vector\n",
        "        shear_ecc = min(shear_ecc, 9)\n",
        "        rot_ecc = min(rot_ecc, 3)\n",
        "        rot_mag = min(rot_mag, 1) * 89.9\n",
        "\n",
        "        shear = np.random.uniform(0.05, shear_mag, shear_ecc)\n",
        "        shear = np.append(shear, np.zeros(9 - shear_ecc))\n",
        "        np.random.shuffle(shear)\n",
        "        (shear[:3])[shear[:3] == 0] = 1\n",
        "\n",
        "        rotation = np.random.uniform(0., rot_mag, rot_ecc)\n",
        "        rotation = np.append(rotation, np.zeros(3 - rot_ecc))\n",
        "        np.random.shuffle(rotation)\n",
        "\n",
        "        affine = np.append(shear, rotation)\n",
        "\n",
        "        params = np.zeros((20, 2))\n",
        "        params[:, 0] = 1\n",
        "        if eccentricity == 0:\n",
        "            params[:8, 0] = 1\n",
        "\n",
        "            return np.append(params.reshape((-1, )), affine)\n",
        "        if eccentricity >= 4:\n",
        "\n",
        "            return np.append(np.random.uniform(0, 1, 40), affine)\n",
        "        cube = params\n",
        "        sphere = params.copy()\n",
        "        cylinder = params.copy()\n",
        "        if eccentricity == 1:\n",
        "            cube[:8, 0] = 1\n",
        "            sphere[:8, :] = 1\n",
        "            ne = random.randint(0, 2)\n",
        "            cylinder[8+(ne*4):12+(ne*4), :] = 1\n",
        "\n",
        "        if eccentricity == 2:\n",
        "            cube[:8, 0] = 1\n",
        "            sphere[:8, 0] = random.uniform(0.05, 1)\n",
        "            sphere[random.randint(0, 7), :] = 1\n",
        "            cylinder[8:20, 0] = random.uniform(0.05, 1)\n",
        "            cylinder[random.randint(0, 12), :] = 1\n",
        "\n",
        "        if eccentricity == 3:\n",
        "            r = random.uniform(0.5, 1)\n",
        "            cube[:8, 0] = r/4\n",
        "            cube[:8, 1] = 1\n",
        "            nc = random.randint(0, 8)\n",
        "            sphere[nc, 0] = r\n",
        "            sphere[nc, 1] = 1\n",
        "            cylinder[8:, 0] = np.random.uniform(0.1, 1, 12)\n",
        "            cylinder[8:, 1] = 1\n",
        "\n",
        "        params = [cube, sphere, cylinder][random.randint(0, 2)].reshape((40, ))\n",
        "        return np.append(params, affine)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USjPj9WqVlh_",
        "colab_type": "text"
      },
      "source": [
        "### Point cloud \n",
        "\n",
        "Draw random points on the mesh and remove 1/3 to represent occlusion"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tz0ZeGncujLE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def shootCloud(V, F, density: int = 256, occlusion = 80, shuffle=False):\n",
        "    cloud, _ = pcu.sample_mesh_random(V, F, np.array([], dtype=V.dtype), num_samples=density+occlusion)\n",
        "\n",
        "    tree = KDTree(cloud)\n",
        "\n",
        "    #find occlusion nearest neighbours and remove\n",
        "    if occlusion > 0:\n",
        "        x = np.random.randint(len(cloud))\n",
        "        _, nearest_ind = tree.query(cloud[x].reshape(-1, 3), k=occlusion)\n",
        "        cloud = np.delete(cloud, nearest_ind, axis=0)\n",
        "\n",
        "    #unorder the data\n",
        "    if shuffle :\n",
        "        np.random.shuffle(cloud)\n",
        "\n",
        "    return cloud"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TFY3eQGMWGHI",
        "colab_type": "text"
      },
      "source": [
        "Manual gradient (reconstrcution loss) requires igl library which is not available on Colab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ul1ynIFk61ka",
        "colab_type": "code",
        "outputId": "b8e9dc15-b050-49c0-d279-2277335b0820",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        }
      },
      "source": [
        "import igl\n",
        "\n",
        "def plasticGradient(V, F, b_params, epsilon=0.001):\n",
        "\n",
        "    block = PlasticBlock(b_params)\n",
        "    Vb, _, Fb = block.get_obj()\n",
        "    loss = igl.hausdorff(V, F, Vb, Fb)\n",
        "    m_gradient = np.zeros((b_params.shape[0],))\n",
        "\n",
        "    for p in range(len(b_params)):\n",
        "        # generate alternate mesh\n",
        "        param = b_params.copy()\n",
        "        param[p] = param[p] + epsilon\n",
        "        block = PlasticBlock(param)\n",
        "        # compute distance\n",
        "        Vb, _, Fb = block.get_obj()\n",
        "        d = igl.hausdorff(V, F, Vb, Fb)\n",
        "        m_gradient[p] = (d - loss)/epsilon\n",
        "\n",
        "    return m_gradient"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-880fc551c433>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0migl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mplasticGradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mV\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mblock\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPlasticBlock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'igl'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k4SO9gzYWcCM",
        "colab_type": "text"
      },
      "source": [
        "## PointNet "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iy7p7rVvWf8e",
        "colab_type": "text"
      },
      "source": [
        "The implementation is in the form of auto encoder.\n",
        "The Classification Network of the original design is plug into what would normally be the Segmentation Network, the encoder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KhKseKo-_IB2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# https://github.com/tensorflow/models/tree/master/research/transformer\n",
        "class SpatialTransformer(nn.Module):\n",
        "    def __init__(self, k=64):\n",
        "        super(SpatialTransformer, self).__init__()\n",
        "        self._cuda = 'cuda' # torch.device('cuda')\n",
        "        self.k = k\n",
        "\n",
        "        self.Conv1 = torch.nn.Conv1d(self.k, self.k, 1)\n",
        "        self.BN1 = nn.BatchNorm1d(self.k)\n",
        "\n",
        "        self.Conv2 = torch.nn.Conv1d(self.k, self.k * 2, 1)\n",
        "        self.BN2 = nn.BatchNorm1d(self.k * 2)\n",
        "\n",
        "        self.Conv16 = torch.nn.Conv1d(self.k * 2, self.k * 16, 1)\n",
        "        self.BN16 = nn.BatchNorm1d(self.k * 16)\n",
        "\n",
        "        self.Mlp16 = nn.Linear(self.k * 16, self.k * 8)\n",
        "        self.BN8 = nn.BatchNorm1d(self.k * 8)\n",
        "\n",
        "        self.Mlp8 = nn.Linear(self.k * 8, self.k * 4)\n",
        "        self.BN4 = nn.BatchNorm1d(self.k * 4)\n",
        "\n",
        "        self.Mlp4 = nn.Linear(self.k * 4, self.k * self.k)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size = x.shape[0]\n",
        "        x = F.relu(self.BN1(self.Conv1(x)))\n",
        "        x = F.relu(self.BN2(self.Conv2(x)))\n",
        "        x = F.relu(self.BN16(self.Conv16(x)))\n",
        "        x = torch.max(x, 2, keepdim=True)[0]\n",
        "        x = x.view(-1, self.k*16)\n",
        "\n",
        "        x = F.relu(self.BN8(self.Mlp16(x)))\n",
        "        x = F.relu(self.BN4(self.Mlp8(x)))\n",
        "        x = self.Mlp4(x)\n",
        "\n",
        "        eye = torch.eye(self.k, requires_grad=True, device=self._cuda).view(1, self.k * self.k).repeat(batch_size, 1)\n",
        "        x = x + eye\n",
        "        x = x.view(-1, self.k, self.k)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "\n",
        "    def __init__(self, m=52, k=64):\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        # number of output parameters\n",
        "        # base depth nb of feature maps\n",
        "        self.k = k\n",
        "\n",
        "        \"\"\"\n",
        "        conv 1d : 64\n",
        "        conv 1d : 128\n",
        "        conv 1d : 1024\n",
        "        maxpooling\n",
        "        Mlp 1024\n",
        "        batch norm (instead of dropout)\n",
        "        Mlp 256\n",
        "        Mlp 52\n",
        "               output x : size 52\n",
        "        \"\"\"\n",
        "\n",
        "        self.Conv1 = torch.nn.Conv1d((16+1)*self.k, self.k*8, 1)\n",
        "        self.BN8 = nn.BatchNorm1d(self.k*8)\n",
        "\n",
        "        self.Conv2 = torch.nn.Conv1d(self.k*8, self.k*4, 1)\n",
        "        self.BN4 = nn.BatchNorm1d(self.k*4)\n",
        "\n",
        "        self.Conv3 = torch.nn.Conv1d(self.k*4, self.k*2, 1)\n",
        "        self.BN2 = nn.BatchNorm1d(self.k*2)\n",
        "\n",
        "        self.Mlp_out = nn.Linear(self.k*2, m)\n",
        "\n",
        "    def forward(self, features, x):\n",
        "        x = torch.cat([features, x], 1)\n",
        "        x = F.relu(self.BN8(self.Conv1(x)))\n",
        "        x = F.relu(self.BN4(self.Conv2(x)))\n",
        "        x = F.relu(self.BN2(self.Conv3(x)))\n",
        "        x = self.Mlp_out(x.transpose(2, 1).contiguous())\n",
        "        x = torch.max(x, 1, keepdim=True)[0]\n",
        "        params = torch.split(torch.squeeze(x), [40, 9, 3], dim=1)\n",
        "        shape = params[0]\n",
        "        affine = params[1]\n",
        "        rotation = params[2]\n",
        "        shape = torch.sigmoid(0.1*shape)\n",
        "        affine = F.relu(affine)\n",
        "        rotation = torch.sigmoid(0.05 * rotation)\n",
        "\n",
        "        x = torch.cat([shape, affine, rotation], 1)\n",
        "           # x.view(batch_size, self.m, 1)\n",
        "\n",
        "        return x\n",
        "\n",
        "def e_init(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('Conv') != -1:\n",
        "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
        "    elif classname.find('Norm') != -1:\n",
        "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
        "        nn.init.constant_(m.bias.data, 0)\n",
        "\n",
        "# Implementation of PointNet\n",
        "# https://arxiv.org/abs/1612.00593\n",
        "class PlasticNet(nn.Module):\n",
        "\n",
        "    def __init__(self, n=256, k=64):\n",
        "        super(PlasticNet, self).__init__()\n",
        "\n",
        "        self.n = n\n",
        "        self.k = k\n",
        "        self.Encoder = Encoder()\n",
        "        self.Transformer = SpatialTransformer(self.k)\n",
        "\n",
        "        self.Encoder.apply(e_init)\n",
        "\n",
        "        self.batch_size = 1\n",
        "\n",
        "        \"\"\"\n",
        "        conv 1d : 64\n",
        "        batch norm (instead of dropout)\n",
        "        transformation\n",
        "        matrix multiplication\n",
        "        conv 1d : 128\n",
        "        conv 1d : 1024\n",
        "        maxpooling\n",
        "        Encoder\n",
        "            output x : size 52\n",
        "        \"\"\"\n",
        "\n",
        "        self.Conv1 = torch.nn.Conv1d(3, self.k, 1)\n",
        "        self.Conv2 = torch.nn.Conv1d(self.k, self.k * 2, 1)\n",
        "        self.Conv16 = torch.nn.Conv1d(self.k * 2, self.k * 16, 1)\n",
        "        self.BN1 = nn.BatchNorm1d(self.k)\n",
        "        self.BN2 = nn.BatchNorm1d(self.k * 2)\n",
        "        self.BN16 = nn.BatchNorm1d(self.k * 16)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Mlp 1\n",
        "        x = F.relu(self.BN1(self.Conv1(x)))\n",
        "        features = x\n",
        "        res = self.Transformer(x)\n",
        "        # T @ F matmul\n",
        "        x = torch.bmm(x.transpose(2, 1), res).transpose(2, 1)\n",
        "\n",
        "        # Mlp 2\n",
        "        x = F.relu(self.BN2(self.Conv2(x)))\n",
        "\n",
        "        # Pooling\n",
        "        x = self.BN16(self.Conv16(x))\n",
        "        x = torch.max(x, 2, keepdim=True)[0]\n",
        "        x = x.view(-1, self.k*16, 1).repeat(1, 1, self.n)\n",
        "\n",
        "        # Encoder\n",
        "        x = self.Encoder(features, x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def loss(self, seed, params):\n",
        "        return ((seed - params)**2).mean()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TVYlt8llXjwU",
        "colab_type": "text"
      },
      "source": [
        "### Training  (Still incomplete)\n",
        "\n",
        "I still don't understand what is not working with my data loader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h8SqflM6fSb_",
        "colab_type": "text"
      },
      "source": [
        "First test should be done on the vertices themselves as the point cloud.\n",
        "The network should be able to overfit the data and get 100% accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2X2dOZ_DtYvs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class BlockDataset(Dataset):\n",
        "    \"\"\"Plastic Block dataset.\"\"\"\n",
        "\n",
        "    def __init__(self, size):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            size of the dataset\n",
        "        \"\"\"\n",
        "        self.block_clouds = []\n",
        "        self.params = []\n",
        "        for i in range(size):\n",
        "            block = PlasticBlock()\n",
        "            V, E, F = block.get_obj()\n",
        "            self.block_clouds.append(shootCloud(V, F))\n",
        "            self.params.append(block.source)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.block_clouds)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "\n",
        "        cloud = self.block_clouds[idx]\n",
        "        cloud = np.array([cloud])\n",
        "        cloud = cloud.astype('float')\n",
        "        \n",
        "        source = self.params[idx]\n",
        "        source = np.array([source])\n",
        "        source = source.astype('float')\n",
        "        sample = {'cloud': cloud, 'source': source}\n",
        "\n",
        "        return sample"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vKzLLzPSbRsG",
        "colab_type": "code",
        "outputId": "2f419140-af5f-41b0-bc7c-233cace4e5fc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "cuda = device.type == 'cuda'\n",
        "print(device, cuda)\n",
        "\n",
        "batch_size = 128\n",
        "\n",
        "train_size = 6400\n",
        "test_size = 1600\n",
        "\n",
        "train_data = BlockDataset(train_size)\n",
        "test_data = BlockDataset(test_size)\n",
        "\n",
        "indices = list(range(len(train_data)))\n",
        "random.shuffle(indices)\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_data,\n",
        "    batch_size=batch_size,\n",
        "    num_workers=4,\n",
        "    pin_memory=cuda\n",
        ")\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    test_data,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    num_workers=4,\n",
        "    pin_memory=cuda,\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda:0 True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Dc6B24uR7Fp",
        "colab_type": "code",
        "outputId": "8d9e58bb-6948-4edf-a4f2-f43190c79efb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "train_loader"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch.utils.data.dataloader.DataLoader at 0x7f3182b3be80>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e3WY6wsIcEsj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model, loader, optimizer, epoch, scheduler):\n",
        "    model.train()\n",
        "    loss_fn = nn.MSELoss()\n",
        "\n",
        "    for batch_idx, batch in enumerate(loader):\n",
        "        inputs = batch.get('cloud').to(device, dtype=torch.float).squeeze().transpose(2, 1)\n",
        "        target = batch.get('source').to(device, dtype=torch.float).squeeze()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output = model(inputs)\n",
        "        loss = loss_fn(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if batch_idx % 100 == 0:\n",
        "            print(model.__class__.__name__, ' Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch + 1, batch_idx * len(inputs), len(loader) * len(inputs),\n",
        "                100. * batch_idx / len(loader), loss.item()))\n",
        "\n",
        "    scheduler.step()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ctdVjqTcUWN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test(model, loader, dset='test'):\n",
        "    model.eval()\n",
        "    test_loss_fn = nn.MSELoss()\n",
        "\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    test_size = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, batch in enumerate(loader):\n",
        "            inputs = batch.get('cloud').to(device, dtype=torch.float).squeeze().transpose(2, 1)\n",
        "            target = batch.get('source').to(device, dtype=torch.float).squeeze()\n",
        "\n",
        "            output = model(inputs)\n",
        "            test_size += len(inputs)\n",
        "            test_loss += test_loss_fn(output, target).item()\n",
        "\n",
        "    test_loss /= test_size\n",
        "    if dset == \"validation\":\n",
        "        print('Validation set: Average loss: {:.4f} over {} samples\\n'.format(\n",
        "            test_loss, test_size))\n",
        "    if dset == \"train\":\n",
        "        print('Train set: Average loss: {:.4f} over {} samples\\n'.format(\n",
        "            test_loss, test_size))\n",
        "    if dset == \"test\":\n",
        "        print('Test set: Average loss: {:.4f} over {} samples\\n'.format(\n",
        "            test_loss, test_size))\n",
        "\n",
        "    return test_loss\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AFT3E9SWcWJt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "\n",
        "def weight_reset(m):\n",
        "    if isinstance(m, nn.Conv1d) or isinstance(m, nn.Linear):\n",
        "        m.reset_parameters()\n",
        "\n",
        "\n",
        "def MultipleRun(runs, epochs, model):\n",
        "    model = model.to(device)\n",
        "    name = model.__class__.__name__\n",
        "\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
        "    exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=2, gamma=1.0)\n",
        "\n",
        "    results = {'name': name, 'loss': [0] * epochs}\n",
        "    savefile = os.path.join(savedir, results['name'] + '.pkl')\n",
        "\n",
        "    best_net = 'best_' + name\n",
        "\n",
        "    for run in range(runs):\n",
        "        print(\"\\n\", \"  --  Run : \", run + 1)\n",
        "        since = time.time()\n",
        "        best_loss = float('inf')\n",
        "\n",
        "        # Reseting all weights for new run\n",
        "        model.apply(weight_reset)\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "\n",
        "            train(model, train_loader, optimizer, epoch, exp_lr_scheduler)\n",
        "            loss = test(model, test_loader)\n",
        "\n",
        "            if loss < best_loss:\n",
        "                best_loss = loss\n",
        "\n",
        "                # Saving best model parameters\n",
        "                torch.save({\n",
        "                    'epoch_based0': epoch,\n",
        "                    'state_dict': model.state_dict(),\n",
        "                    'loss': loss,\n",
        "                    'optimizer': optimizer.state_dict(),\n",
        "                }, best_net)\n",
        "\n",
        "                print('new loss parameters saved {}'.format(best_loss))\n",
        "\n",
        "            # Updating average results\n",
        "            results['loss'][epoch] += loss / runs\n",
        "\n",
        "        time_elapsed = time.time() - since\n",
        "        print('Training complete in {:.0f}m {:.0f}s'.format(\n",
        "            time_elapsed // 60, time_elapsed % 60))\n",
        "\n",
        "    with open(savefile, 'wb') as fout:\n",
        "        pickle.dump(results, fout)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FA3DnYNpAS29",
        "colab_type": "code",
        "outputId": "0186f172-237b-4fa5-e35e-ec4f289a9fe7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 703
        }
      },
      "source": [
        "#Number of parameters\n",
        "from torchsummary import summary\n",
        "\n",
        "model = PlasticNet().to(device)\n",
        "\n",
        "print(\"PlasticNet : \",sum(p.numel() for p in model.parameters() if p.requires_grad))\n",
        "summary(model, (3, 256))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "PlasticNet :  2730740\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv1d-1              [-1, 64, 256]             256\n",
            "       BatchNorm1d-2              [-1, 64, 256]             128\n",
            "            Conv1d-3              [-1, 64, 256]           4,160\n",
            "       BatchNorm1d-4              [-1, 64, 256]             128\n",
            "            Conv1d-5             [-1, 128, 256]           8,320\n",
            "       BatchNorm1d-6             [-1, 128, 256]             256\n",
            "            Conv1d-7            [-1, 1024, 256]         132,096\n",
            "       BatchNorm1d-8            [-1, 1024, 256]           2,048\n",
            "            Linear-9                  [-1, 512]         524,800\n",
            "      BatchNorm1d-10                  [-1, 512]           1,024\n",
            "           Linear-11                  [-1, 256]         131,328\n",
            "      BatchNorm1d-12                  [-1, 256]             512\n",
            "           Linear-13                 [-1, 4096]       1,052,672\n",
            "SpatialTransformer-14               [-1, 64, 64]               0\n",
            "           Conv1d-15             [-1, 128, 256]           8,320\n",
            "      BatchNorm1d-16             [-1, 128, 256]             256\n",
            "           Conv1d-17            [-1, 1024, 256]         132,096\n",
            "      BatchNorm1d-18            [-1, 1024, 256]           2,048\n",
            "           Conv1d-19             [-1, 512, 256]         557,568\n",
            "      BatchNorm1d-20             [-1, 512, 256]           1,024\n",
            "           Conv1d-21             [-1, 256, 256]         131,328\n",
            "      BatchNorm1d-22             [-1, 256, 256]             512\n",
            "           Conv1d-23             [-1, 128, 256]          32,896\n",
            "      BatchNorm1d-24             [-1, 128, 256]             256\n",
            "           Linear-25              [-1, 256, 52]           6,708\n",
            "          Encoder-26                   [-1, 52]               0\n",
            "================================================================\n",
            "Total params: 2,730,740\n",
            "Trainable params: 2,730,740\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 13.18\n",
            "Params size (MB): 10.42\n",
            "Estimated Total Size (MB): 23.60\n",
            "----------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B3wiRSxgCh6y",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ETY-LIIuDtkk",
        "colab_type": "code",
        "outputId": "54323463-d526-422c-f4cb-4a254e66c587",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 754
        }
      },
      "source": [
        "savedir = 'results-PN'\n",
        "if not os.path.exists(savedir):\n",
        "    os.makedirs(savedir)\n",
        "\n",
        "MultipleRun(1,10,model)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "   --  Run :  1\n",
            "PlasticNet  Epoch: 1 [0/6400 (0%)]\tLoss: 0.225487\n",
            "Test set: Average loss: 0.0015 over 1600 samples\n",
            "\n",
            "new loss parameters saved 0.0014937232993543147\n",
            "PlasticNet  Epoch: 2 [0/6400 (0%)]\tLoss: 0.183480\n",
            "Test set: Average loss: 0.0015 over 1600 samples\n",
            "\n",
            "new loss parameters saved 0.0014780331775546074\n",
            "PlasticNet  Epoch: 3 [0/6400 (0%)]\tLoss: 0.180531\n",
            "Test set: Average loss: 0.0015 over 1600 samples\n",
            "\n",
            "new loss parameters saved 0.0014631824567914009\n",
            "PlasticNet  Epoch: 4 [0/6400 (0%)]\tLoss: 0.178431\n",
            "Test set: Average loss: 0.0015 over 1600 samples\n",
            "\n",
            "new loss parameters saved 0.001454295516014099\n",
            "PlasticNet  Epoch: 5 [0/6400 (0%)]\tLoss: 0.177174\n",
            "Test set: Average loss: 0.0014 over 1600 samples\n",
            "\n",
            "new loss parameters saved 0.0014466885104775428\n",
            "PlasticNet  Epoch: 6 [0/6400 (0%)]\tLoss: 0.176128\n",
            "Test set: Average loss: 0.0014 over 1600 samples\n",
            "\n",
            "new loss parameters saved 0.0014386493060737848\n",
            "PlasticNet  Epoch: 7 [0/6400 (0%)]\tLoss: 0.175118\n",
            "Test set: Average loss: 0.0014 over 1600 samples\n",
            "\n",
            "new loss parameters saved 0.0014317239355295898\n",
            "PlasticNet  Epoch: 8 [0/6400 (0%)]\tLoss: 0.174180\n",
            "Test set: Average loss: 0.0014 over 1600 samples\n",
            "\n",
            "new loss parameters saved 0.0014250975474715234\n",
            "PlasticNet  Epoch: 9 [0/6400 (0%)]\tLoss: 0.173256\n",
            "Test set: Average loss: 0.0014 over 1600 samples\n",
            "\n",
            "new loss parameters saved 0.0014177033957093953\n",
            "PlasticNet  Epoch: 10 [0/6400 (0%)]\tLoss: 0.172334\n",
            "Test set: Average loss: 0.0014 over 1600 samples\n",
            "\n",
            "new loss parameters saved 0.0014097803458571435\n",
            "Training complete in 1m 11s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6-hyVG8lbOV5",
        "colab_type": "text"
      },
      "source": [
        "### Summary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uEmoWkKAXli_",
        "colab_type": "code",
        "outputId": "9a828d53-f374-49aa-f9e4-5b2c8c5bd573",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        }
      },
      "source": [
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 6))\n",
        "\n",
        "for filename in os.listdir(savedir):\n",
        "    if filename.endswith('.pkl'):\n",
        "        with open(os.path.join(savedir, filename),'rb') as fin:\n",
        "            results = pickle.load(fin)\n",
        "            ax1.plot(results['loss'])\n",
        "            ax1.set_ylabel('accuray')\n",
        "            ax1.set_xlabel('epochs')\n",
        "            \n",
        "plt.legend()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-dfeeeb90512f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0max1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msavedir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.pkl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msavedir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QH2ogqGKa_z_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}